<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Projects | Suharssh</title>

    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <header class="site-header">
        <div class="header-content">
            <h1>Suharssh</h1>
            <nav>
                <a href="index.html">Home</a>
                <a href="projects.html">Projects</a>
                <a href="index.html#contact">Contact</a>
            </nav>
        </div>
    </header>

    <main class="page-container">

        <h2>Projects</h2>
        <p>
            A selection of academic, research, and engineering projects focused on machine learning,
            deep learning, multimodal analysis, and applied AI systems.
        </p>

        <div class="project-card">
            <h3>Sleep Stage Analysis with Deep Learning & Explainable AI</h3>
            <p>
                Final year research project on automated sleep stage classification using EEG/PSG data.
                Implemented CNN, LSTM, DNN, and Transformer-based architectures, and integrated
                explainability techniques such as SHAP and LIME. Knowledge graphs were used to enhance
                interpretability and reasoning over sleep patterns.
            </p>
            <a href="https://github.com/Suharssh" target="_self">View on GitHub →</a>
        </div>

        <div class="project-card">
            <h3>V.A.R.Y.S — Video Analysis for Real-Time Yield of Sentiments</h3>
            <p>
                Designed a multimodal AI system to perform context-aware, real-time analysis of social
                media comment sections. V.A.R.Y.S integrates video, audio, metadata, and user comments
                to derive stance, sentiment depth, demographic patterns, and statistical summaries.
                The pipeline leverages transformers, weak supervision, and generative models to move
                beyond binary sentiment classification and capture nuanced, context-dependent meaning.
            </p>
            <p>
                Applied to real-world cases such as political unrest, media controversies, and social
                movements, the system demonstrates how content, creator influence, and context affect
                user behavior and opinion formation at scale.
            </p>
            <a href="https://github.com/Suharssh" target="_self">View on GitHub →</a>
        </div>

        <div class="project-card">
            <h3>Detection and Classification of Cotton Plant Leaf Diseases</h3>
            <p>
                Developed deep learning models for automated detection and classification of diseases
                in cotton plant leaves. The project focused on image preprocessing, data augmentation,
                CNN-based architectures, and performance evaluation for practical agricultural use cases.
            </p>
            <a href="https://github.com/Suharssh" target="_self">View on GitHub →</a>
        </div>

        <div class="project-card">
            <h3>Taxi Trip Duration Prediction</h3>
            <p>
                Built a machine learning model to predict taxi trip durations using structured trip data.
                Performed feature engineering and model evaluation to analyze the impact of temporal,
                geographic, and demand-related factors on travel time.
            </p>
            <a href="https://github.com/Suharssh" target="_self">View on GitHub →</a>
        </div>

        <div class="project-card">
            <h3>Air Piano — Gesture-Based Musical Instrument using Flex Sensors</h3>
            <p>
                Designed and implemented a gesture-controlled musical instrument using an Arduino Uno and
                flex sensors integrated into a wearable glove. Finger bending gestures are captured via
                analog sensor readings and mapped to distinct musical tones, enabling users to play music
                without physical keys.
            </p>
            <p>
                The system uses three flex sensors, each capable of producing two different tones based on
                bending thresholds, increasing the expressive range of the instrument. Sensor signals are
                processed in real time and communicated to a software layer for audio playback, combining
                hardware sensing with signal interpretation and human–computer interaction.
            </p>
            <p>
                Beyond entertainment, the project explores accessibility use cases, where simple hand
                gestures can be used by elderly or physically impaired users to trigger audio cues or alerts.
            </p>
            <a href="https://github.com/Suharssh" target="_self">View on GitHub →</a>
        </div>

        <div style="margin-top: 40px;">
            <a href="index.html">← Back to Home</a>
        </div>

    </main>

</body>
</html>
